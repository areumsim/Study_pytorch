{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-z_H2MehhPz"
      },
      "source": [
        "# 텐서 - 책 3장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPwtjhM-hnOj",
        "outputId": "cd6b191e-bf49-4426-b9f7-21dffcf3080d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1.0, 2.0, 3.0]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "a = [1.0, 2.0, 3.0]\n",
        "\n",
        "torch.ones(3)\n",
        "torch.zeros(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9_3uh29hnWR",
        "outputId": "946e1f97-2589-434f-e7b0-b486ec83c618"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.]])\n",
            "torch.Size([2, 5])\n",
            "\n",
            "tensor([[[0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0.]]])\n",
            "torch.Size([1, 2, 5])\n"
          ]
        }
      ],
      "source": [
        "a = torch.zeros([2,5])\n",
        "print(a)\n",
        "print(a.shape)\n",
        "print()\n",
        "\n",
        "a = a[None] #길이가1인 차원을 추가함, unsqueeze\n",
        "print(a)\n",
        "print(a.shape)\n",
        "print()\n",
        "\n",
        "a = a[None] #길이가1인 차원을 추가함, unsqueeze\n",
        "print(a)\n",
        "print(a.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nb3NZy-BhEYe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7sKIqnyhkBd"
      },
      "source": [
        "# 1. Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3M9B6RNdaYBW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = pd.read_csv(\"/content/regression.csv\")\n",
        "\n",
        "x = data['x']\n",
        "y = data['y']\n",
        "\n",
        "x = torch.tensor(x, requires_grad=True)\n",
        "y = torch.tensor(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "Bbc3cUoLmrQl",
        "outputId": "01b4b6df-ef53-412b-c273-b2c624706f46"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.scatter(x.detach().numpy(), y.detach().numpy(), s=2)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.show()\n",
        "\n",
        "#####  ?? detach().numpy()\n",
        "# ref. https://byeongjo-kim.tistory.com/32\n",
        "# .detach() : \n",
        "  # \"Returns a new Tensor, detached from the current graph. The result will never require gradient.\" \n",
        "  # 즉 graph에서 분리한 새로운 tensor를 리턴한다.\n",
        "  # detach()는 이 연산 기록으로 부터 분리한 tensor을 반환\n",
        "\n",
        "# .numpy() :\n",
        "  # tensor를 numpy로 변환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAUMNvo9eMC6",
        "outputId": "cbf6999f-59d2-4096-f271-56a4cb378513"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch:000, w:0.6176, b:0.2987, loss:44.3576 \n",
            "epoch:050, w:1.7507, b:0.4935, loss:15.3416 \n",
            "epoch:100, w:2.3492, b:0.7062, loss:7.8862 \n",
            "epoch:150, w:2.5635, b:0.8984, loss:6.3209 \n",
            "epoch:200, w:2.6393, b:1.0721, loss:5.6222 \n",
            "epoch:250, w:2.6660, b:1.2289, loss:5.1245 \n",
            "epoch:300, w:2.6755, b:1.3705, loss:4.7277 \n",
            "epoch:350, w:2.6788, b:1.4984, loss:4.4052 \n",
            "epoch:400, w:2.6800, b:1.6139, loss:4.1423 \n",
            "epoch:450, w:2.6804, b:1.7181, loss:3.9280 \n",
            "epoch:500, w:2.6805, b:1.8123, loss:3.7532 \n",
            "epoch:550, w:2.6806, b:1.8974, loss:3.6106 \n",
            "epoch:600, w:2.6806, b:1.9742, loss:3.4944 \n",
            "epoch:650, w:2.6806, b:2.0435, loss:3.3996 \n",
            "epoch:700, w:2.6806, b:2.1061, loss:3.3223 \n",
            "epoch:750, w:2.6806, b:2.1627, loss:3.2592 \n",
            "epoch:800, w:2.6806, b:2.2138, loss:3.2078 \n",
            "epoch:850, w:2.6806, b:2.2599, loss:3.1659 \n",
            "epoch:900, w:2.6806, b:2.3015, loss:3.1317 \n",
            "epoch:950, w:2.6806, b:2.3391, loss:3.1038 \n",
            "epoch:1000, w:2.6806, b:2.3731, loss:3.0811 \n",
            "epoch:1050, w:2.6806, b:2.4038, loss:3.0625 \n",
            "epoch:1100, w:2.6806, b:2.4315, loss:3.0474 \n",
            "epoch:1150, w:2.6806, b:2.4565, loss:3.0351 \n",
            "epoch:1200, w:2.6806, b:2.4791, loss:3.0250 \n",
            "epoch:1250, w:2.6806, b:2.4995, loss:3.0168 \n",
            "epoch:1300, w:2.6806, b:2.5179, loss:3.0101 \n",
            "epoch:1350, w:2.6806, b:2.5345, loss:3.0047 \n",
            "epoch:1400, w:2.6806, b:2.5496, loss:3.0002 \n",
            "epoch:1450, w:2.6806, b:2.5631, loss:2.9966 \n",
            "epoch:1500, w:2.6806, b:2.5754, loss:2.9936 \n",
            "epoch:1550, w:2.6806, b:2.5865, loss:2.9912 \n",
            "epoch:1600, w:2.6806, b:2.5964, loss:2.9893 \n",
            "epoch:1650, w:2.6806, b:2.6055, loss:2.9876 \n",
            "epoch:1700, w:2.6806, b:2.6136, loss:2.9863 \n",
            "epoch:1750, w:2.6806, b:2.6210, loss:2.9853 \n",
            "epoch:1800, w:2.6806, b:2.6276, loss:2.9844 \n",
            "epoch:1850, w:2.6806, b:2.6336, loss:2.9837 \n",
            "epoch:1900, w:2.6806, b:2.6390, loss:2.9831 \n",
            "epoch:1950, w:2.6806, b:2.6439, loss:2.9826 \n"
          ]
        }
      ],
      "source": [
        "# w,b 모두 uniform(0,1) initializing \n",
        "#w 정의\n",
        "w = torch.rand(1, dtype=float, requires_grad=True)\n",
        "\n",
        "#b 정의\n",
        "b = torch.rand(1, 1, dtype=float, requires_grad=True)\n",
        "\n",
        "# # y=wx+b\n",
        "# def func(w, b):\n",
        "#   return ( x*w + b )\n",
        "\n",
        "# loss function 정의\n",
        "# loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# optim method 정의\n",
        "optim = torch.optim.SGD([w, b], lr=1e-4, momentum=0.9)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#####  #####  #####\n",
        "EPOCH = 2000\n",
        "\n",
        "tr_epoch = []\n",
        "tr_loss = []\n",
        "tr_w = []\n",
        "tr_b = []\n",
        "\n",
        "# optimization 과정\n",
        "for epoch in range(EPOCH):\n",
        "  # y=wx+b\n",
        "  yhat = ( x*w + b )\n",
        "\n",
        "  # loss function 정의\n",
        "  loss = ((y - yhat)**2).mean()\n",
        "  \n",
        "  # 변화도를 0으로, 역전파 수행, 가중치 갱신\n",
        "  optim.zero_grad()  # 모델 매개변수의 변화도를 재설정\n",
        "  loss.backward() # 예측 손실(prediction loss)을 역전파, 각 매개변수에 대한 손실의 변화도를 저장\n",
        "  optim.step()  # 수집된 변화도로 매개변수를 조정\n",
        "\n",
        "  if epoch % 50 == 0 :\n",
        "    print('epoch:{:03d}, w:{:.4f}, b:{:.4f}, loss:{:.4f} '.format(epoch, w.item(), b.item(), loss.item()))\n",
        "    tr_epoch.append(epoch)\n",
        "    tr_loss.append(loss.item())\n",
        "    tr_w.append(w.item())\n",
        "    tr_b.append(b.item())\n",
        "\n",
        "\n",
        "### ?? zero_grad() 를 항상 backward() 전에 해야하는가\n",
        "  # zero_grad() :  method는 optimizer에 연결된 parameter들의 gradient를 0으로 만든다.\n",
        "  # \"Pytorch에서는 gradients값들을 추후에 backward를 해줄때 계속 더해주기 때문\"에,\n",
        "  # 우리는 항상 backpropagation을 하기전에 gradients를 zero로 만들어주고 시작을 해야한다.\n",
        "# optim.zero_grad() \n",
        "# loss.backward() \n",
        "# optim.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "wIVNIyNAaOqH",
        "outputId": "f14feebb-ef12-4cc0-e1f2-85c2cc7e77d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "w :  2.680617046647237\n",
            "b :  2.6482618335229353\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 최종 w,b \n",
        "print(\"w : \", w.item())\n",
        "print(\"b : \", b.item())\n",
        "\n",
        "plt.plot(tr_epoch, tr_loss)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.scatter(x.detach().numpy(), y.detach().numpy(), s=1)\n",
        "plt.plot(x.detach().numpy(), yhat[-1].detach().numpy(), \"r\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_bseDK6yGb7"
      },
      "source": [
        "# 1. Regression - 같은 문제를 OLS로 풀기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQUkPxE5YQMz",
        "outputId": "babc16c4-21ea-4bdc-8456-e01f10bba28a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# data = pd.read_csv(\"/content/regression.csv\")\n",
        "drive.mount('/content/drive')\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/regression.csv\")\n",
        "\n",
        "\n",
        "x = data['x']\n",
        "y = data['y']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWwx9wvzp-8F",
        "outputId": "c8a17621-8ab2-45bf-bee3-dcab332dccea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "w: 2.68, b: 2.69, loss: 2.980555\n"
          ]
        }
      ],
      "source": [
        "### OLS\n",
        "x_bar = x.mean()\n",
        "y_bar = y.mean()\n",
        "w = ((x - x_bar) * (y - y_bar)).sum() / ((x - x_bar)**2).sum()\n",
        "b = y_bar - w * x_bar\n",
        "\n",
        "yhat = ( x*w + b )\n",
        "loss = ((y - yhat)**2).mean()\n",
        "\n",
        "# 최종 w,b \n",
        "print('w: {:.2f}, b: {:.2f}, loss: {:.6f}'.format(w, b, loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABqDOdcGYQUp",
        "outputId": "4e1ea7b6-9df4-4067-b206-e231aecada9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "w: 2.68, b: 2.69\n"
          ]
        }
      ],
      "source": [
        "### ref. scikit-learn\n",
        "x = data['x'].to_numpy()\n",
        "y = data['y'].to_numpy()\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(x.reshape(-1, 1), y)\n",
        "\n",
        "print('w: {:.2f}, b: {:.2f}'.format(model.coef_[0], model.intercept_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhr_q-pDX3ZV"
      },
      "source": [
        "# 2. MNIST 홀짝 Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q69ULIEGyN87"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transfroms\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oU5WoDVlyOIv",
        "outputId": "181f0fe7-55f3-41eb-a7c1-4e78887b692d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([100, 1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "batch_size = 100\n",
        "\n",
        "# MNIST 데이터셋 로드\n",
        "train_set = torchvision.datasets.MNIST(\n",
        "    root = './data/MNIST',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = transfroms.Compose([\n",
        "        transfroms.ToTensor() # 데이터를 0에서 255까지 있는 값을 0에서 1사이 값으로 변환\n",
        "    ])\n",
        ")\n",
        "test_set = torchvision.datasets.MNIST(\n",
        "    root = './data/MNIST',\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = transfroms.Compose([\n",
        "        transfroms.ToTensor() # 데이터를 0에서 255까지 있는 값을 0에서 1사이 값으로 변환\n",
        "    ])\n",
        ")\n",
        " \n",
        "# train_loader, test_loader 생성\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
        " \n",
        "\n",
        "# input size를 알기 위해서\n",
        "examples = enumerate(train_set)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "print(example_data.shape)\n",
        "\n",
        "examples = enumerate(test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "print(example_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "0JHy9mAs5nvD",
        "outputId": "2ce41a48-39f9-4f9c-e787-56d096eb8d0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "examples = enumerate(test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(3):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URRDSp9h23OY",
        "outputId": "7c56e06e-6328-4283-b398-cd58e2694cd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "5\n"
          ]
        }
      ],
      "source": [
        "x = train_set[0][0]\n",
        "y = train_set[0][1]\n",
        "print(x.shape)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85JsCyRvyOO1",
        "outputId": "f74cfc04-6479-4af1-a242-60b9e99dcc6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.linear_relu_stack(x)\n",
        "        return x\n",
        "\n",
        "model = Net()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_QucvHv9lGG",
        "outputId": "7e582740-211b-4ffd-a568-0b4203bd60c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [59900/60000 (100%)]\tLoss: 0.541839\tAcc: 0.830000\n",
            "Test set: Avg. loss: 0.0054, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 1 [59900/60000 (100%)]\tLoss: 0.478288\tAcc: 0.830000\n",
            "Test set: Avg. loss: 0.0047, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 2 [59900/60000 (100%)]\tLoss: 0.441052\tAcc: 0.830000\n",
            "Test set: Avg. loss: 0.0044, Accuracy: 4926/10000 (49%), (ACC:0.770000)\n",
            "\n",
            "Train Epoch: 3 [59900/60000 (100%)]\tLoss: 0.416520\tAcc: 0.840000\n",
            "Test set: Avg. loss: 0.0042, Accuracy: 4926/10000 (49%), (ACC:0.760000)\n",
            "\n",
            "Train Epoch: 4 [59900/60000 (100%)]\tLoss: 0.399056\tAcc: 0.840000\n",
            "Test set: Avg. loss: 0.0040, Accuracy: 4926/10000 (49%), (ACC:0.750000)\n",
            "\n",
            "Train Epoch: 5 [59900/60000 (100%)]\tLoss: 0.385937\tAcc: 0.840000\n",
            "Test set: Avg. loss: 0.0039, Accuracy: 4926/10000 (49%), (ACC:0.750000)\n",
            "\n",
            "Train Epoch: 6 [59900/60000 (100%)]\tLoss: 0.375684\tAcc: 0.840000\n",
            "Test set: Avg. loss: 0.0038, Accuracy: 4926/10000 (49%), (ACC:0.750000)\n",
            "\n",
            "Train Epoch: 7 [59900/60000 (100%)]\tLoss: 0.367426\tAcc: 0.840000\n",
            "Test set: Avg. loss: 0.0037, Accuracy: 4926/10000 (49%), (ACC:0.740000)\n",
            "\n",
            "Train Epoch: 8 [59900/60000 (100%)]\tLoss: 0.360613\tAcc: 0.840000\n",
            "Test set: Avg. loss: 0.0036, Accuracy: 4926/10000 (49%), (ACC:0.750000)\n",
            "\n",
            "Train Epoch: 9 [59900/60000 (100%)]\tLoss: 0.354882\tAcc: 0.840000\n",
            "Test set: Avg. loss: 0.0036, Accuracy: 4926/10000 (49%), (ACC:0.760000)\n",
            "\n",
            "Train Epoch: 10 [59900/60000 (100%)]\tLoss: 0.349982\tAcc: 0.850000\n",
            "Test set: Avg. loss: 0.0035, Accuracy: 4926/10000 (49%), (ACC:0.760000)\n",
            "\n",
            "Train Epoch: 11 [59900/60000 (100%)]\tLoss: 0.345735\tAcc: 0.850000\n",
            "Test set: Avg. loss: 0.0035, Accuracy: 4926/10000 (49%), (ACC:0.760000)\n",
            "\n",
            "Train Epoch: 12 [59900/60000 (100%)]\tLoss: 0.342010\tAcc: 0.850000\n",
            "Test set: Avg. loss: 0.0035, Accuracy: 4926/10000 (49%), (ACC:0.760000)\n",
            "\n",
            "Train Epoch: 13 [59900/60000 (100%)]\tLoss: 0.338709\tAcc: 0.860000\n",
            "Test set: Avg. loss: 0.0034, Accuracy: 4926/10000 (49%), (ACC:0.760000)\n",
            "\n",
            "Train Epoch: 14 [59900/60000 (100%)]\tLoss: 0.335757\tAcc: 0.860000\n",
            "Test set: Avg. loss: 0.0034, Accuracy: 4926/10000 (49%), (ACC:0.760000)\n",
            "\n",
            "Train Epoch: 15 [59900/60000 (100%)]\tLoss: 0.333096\tAcc: 0.860000\n",
            "Test set: Avg. loss: 0.0034, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 16 [59900/60000 (100%)]\tLoss: 0.330680\tAcc: 0.860000\n",
            "Test set: Avg. loss: 0.0034, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 17 [59900/60000 (100%)]\tLoss: 0.328473\tAcc: 0.860000\n",
            "Test set: Avg. loss: 0.0033, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 18 [59900/60000 (100%)]\tLoss: 0.326443\tAcc: 0.870000\n",
            "Test set: Avg. loss: 0.0033, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 19 [59900/60000 (100%)]\tLoss: 0.324568\tAcc: 0.870000\n",
            "Test set: Avg. loss: 0.0033, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 20 [59900/60000 (100%)]\tLoss: 0.322827\tAcc: 0.870000\n",
            "Test set: Avg. loss: 0.0033, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 21 [59900/60000 (100%)]\tLoss: 0.321204\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0033, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 22 [59900/60000 (100%)]\tLoss: 0.319684\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0032, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 23 [59900/60000 (100%)]\tLoss: 0.318255\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0032, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 24 [59900/60000 (100%)]\tLoss: 0.316907\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0032, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 25 [59900/60000 (100%)]\tLoss: 0.315632\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0032, Accuracy: 4926/10000 (49%), (ACC:0.800000)\n",
            "\n",
            "Train Epoch: 26 [59900/60000 (100%)]\tLoss: 0.314423\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0032, Accuracy: 4926/10000 (49%), (ACC:0.800000)\n",
            "\n",
            "Train Epoch: 27 [59900/60000 (100%)]\tLoss: 0.313271\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0032, Accuracy: 4926/10000 (49%), (ACC:0.800000)\n",
            "\n",
            "Train Epoch: 28 [59900/60000 (100%)]\tLoss: 0.312173\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0032, Accuracy: 4926/10000 (49%), (ACC:0.800000)\n",
            "\n",
            "Train Epoch: 29 [59900/60000 (100%)]\tLoss: 0.311124\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0032, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 30 [59900/60000 (100%)]\tLoss: 0.310118\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0032, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 31 [59900/60000 (100%)]\tLoss: 0.309152\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0032, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 32 [59900/60000 (100%)]\tLoss: 0.308223\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 33 [59900/60000 (100%)]\tLoss: 0.307328\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 34 [59900/60000 (100%)]\tLoss: 0.306464\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 35 [59900/60000 (100%)]\tLoss: 0.305629\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 36 [59900/60000 (100%)]\tLoss: 0.304820\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 37 [59900/60000 (100%)]\tLoss: 0.304037\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 38 [59900/60000 (100%)]\tLoss: 0.303276\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 39 [59900/60000 (100%)]\tLoss: 0.302538\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 40 [59900/60000 (100%)]\tLoss: 0.301819\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 41 [59900/60000 (100%)]\tLoss: 0.301119\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 42 [59900/60000 (100%)]\tLoss: 0.300438\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 43 [59900/60000 (100%)]\tLoss: 0.299773\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 44 [59900/60000 (100%)]\tLoss: 0.299124\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 45 [59900/60000 (100%)]\tLoss: 0.298490\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 46 [59900/60000 (100%)]\tLoss: 0.297870\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 47 [59900/60000 (100%)]\tLoss: 0.297264\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 48 [59900/60000 (100%)]\tLoss: 0.296670\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 49 [59900/60000 (100%)]\tLoss: 0.296089\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.001\n",
        "epochs = 50\n",
        "\n",
        "log_interval = 100\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "train_acc = 0\n",
        "for epoch in range(epochs) :\n",
        "  for batch_idx, (data, y) in enumerate(train_loader):\n",
        "    data = Variable(data.view(-1,28 * 28))\n",
        "    y = Variable(y.view(-1))\n",
        "    y = (y%2).float()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(data)\n",
        "    loss = loss_fn(y_pred.view_as(y), y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # if batch_idx % (log_interval*50) == 0  : #  epoch % log_interval == 0 & \n",
        "    #   y_lb = (y_pred.data>0.5)*1\n",
        "    #   train_acc = y_lb.eq(y.view_as(y_lb)).sum() / len(y_lb)\n",
        "  \n",
        "    #   print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAcc: {:.6f}'.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item(), train_acc )) \n",
        "    #   train_losses.append(loss.item())\n",
        "  \n",
        "  y_lb = (y_pred.data>0.5)*1\n",
        "  train_acc = y_lb.eq(y.view_as(y_lb)).sum() / len(y_lb)\n",
        "  print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAcc: {:.6f}'.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item(), train_acc )) \n",
        "  train_losses.append(loss.item())\n",
        "\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  train_acc = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data = Variable(data.view(-1,28 * 28))\n",
        "      target = Variable(target.view(-1))\n",
        "      target = (target%2).float()\n",
        "      \n",
        "      y_pred = model(data)\n",
        "      y_lb = (y_pred.data>0.5)*1\n",
        "\n",
        "      test_loss += loss_fn(y_pred.view_as(target), target)\n",
        "      pred = y_pred.data.max(1, keepdim=True)[1]\n",
        "      \n",
        "      train_acc = y_lb.eq(target.view_as(y_lb)).sum() / len(y_lb)\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_losses.append(test_loss)\n",
        "  print('Test set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), (ACC:{:.6f})\\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset), train_acc))\n",
        "\n",
        "\n",
        "  # test_loss = 0\n",
        "  # correct = 0\n",
        "  # with torch.no_grad():\n",
        "  #   for data, target in test_loader:\n",
        "  #     data = Variable(data.view(-1,28 * 28))\n",
        "  #     target = Variable(target.view(-1))\n",
        "  #     target = (target%2).float()\n",
        "      \n",
        "  #     y_pred = model(data)\n",
        "  #     test_loss += loss_fn(y_pred.view_as(target), target)\n",
        "  #     pred = y_pred.data.max(1, keepdim=True)[1]\n",
        "  #     correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  # test_loss /= len(test_loader.dataset)\n",
        "  # test_losses.append(test_loss)\n",
        "  # print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8TT7l2L11uI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_Is9ZZxM5JO"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3rXIgXpNEsv",
        "outputId": "d88a33d8-dba5-4463-f1ee-0db66dc494f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Avg. loss: 0.0027, Accuracy: 4926/10000 (49%), (ACC:0.8299999833106995)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_loss = 0\n",
        "correct = 0\n",
        "train_acc = 0\n",
        "with torch.no_grad():\n",
        "  for data, target in test_loader:\n",
        "    data = Variable(data.view(-1,28 * 28))\n",
        "    target = Variable(target.view(-1))\n",
        "    target = (target%2).float()\n",
        "    \n",
        "    y_pred = model(data)\n",
        "    y_lb = (y_pred.data>0.5)*1\n",
        "\n",
        "    test_loss += loss_fn(y_pred.view_as(target), target)\n",
        "    pred = y_pred.data.max(1, keepdim=True)[1]\n",
        "    \n",
        "    train_acc = y_lb.eq(target.view_as(y_lb)).sum() / len(y_lb)\n",
        "    correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "test_loss /= len(test_loader.dataset)\n",
        "test_losses.append(test_loss)\n",
        "print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), (ACC:{})\\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset), train_acc))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "oBEg8Lwd92Gn",
        "outputId": "42947b66-7f26-42fd-fb01-ed2c36ea6546"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f8533dea690>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUt0lEQVR4nO3dXWxd5ZUG4Hc5f+QHO3/ENsG0oUEiaBB0ZKGRikaMqqkoN9AbVJAqRkKTXhSplXpRRC/KJRpNW/ViVCkdUNNRh6qiRXCBZsqgSqgSqmIgQ8LPkBAciOPYSZw/SCCxvebCJ5UB7/d1zz4+5yjf+0iR7bO89/m891459ll7fV9kJszsytfT6QGYWXs42c0K4WQ3K4ST3awQTnazQixv55OtXr06e3t7K+OqMsDiPT31/t9S28/OzlbG1LgjgsbrVkTY/tXPVXfsCtu/2jc75ovZnsXVvtVxm5mZqbV9nWuZPfe5c+dw4cKFBX/wWskeEXcB+BmAZQD+PTMfZ9/f29uLBx54oDI+PT1Nn+/SpUuVsTVr1tBt1clV23/44YdNjQsAVq9eTeMff/wxjS9btozG2UW9du1aum3di15hF6ba98WLF2l8xYoVNM6Omzrmq1atovEzZ87Q+NVXX03j7Jq56qqr6LanT5+ujD399NOVsabPZEQsA/BvAL4O4GYA90fEzc3uz8yWVp3/tm8HcDAzD2XmRQC/AXBPa4ZlZq1WJ9m3Avhg3tdHGo99SkTsjIiRiBi5cOFCjaczszqW/N34zNyVmcOZOaz+djWzpVMn2ccADM37+rrGY2bWheok+x4AN0bEtohYCeCbAJ5rzbDMrNWaLr1l5nREPAzgvzFXensyM99g20QELZeoEtPy5dXDVaUURf2JwUpYqlZ99uxZGu/r66NxVdpjx1SViFT5auXKlTSuzhn72dUx/+STT2j8/PnzNM5Ke6q8pcrA6pyrkibbXj03u17Y+ahVZ8/M5wE8X2cfZtYevl3WrBBOdrNCONnNCuFkNyuEk92sEE52s0K0tZ89ImrVylkr57p16+i2qnap+pPZ9qoVU9WiT548SePqZ/voo48qY3V75ScnJ2lc9ZSzOr2qo6vrQbWRnjt3jsYZdT2oOr26Jtj+1Tlj90bQPnm6VzO7YjjZzQrhZDcrhJPdrBBOdrNCONnNCtHW0tvs7CxtS1TlDlbCUm2gqlSiyh3sudUsqaqExMqRAHDs2DEaV22ojCr71dk3wI+rar9VbaJsllWg+RKV2hbQ5VR1PbJrXf3czU5r7ld2s0I42c0K4WQ3K4ST3awQTnazQjjZzQrhZDcrRNtbXNnqmKrOzmr0qt1RtWKqaYnZVNKqzl53WuKBgQEa37x5c2WMrT67mOdW9yeoVWJZnV7tW62sq447O6dq36pOrmr8qsV1amqqMqbuP2DHlC7fTfdqZlcMJ7tZIZzsZoVwspsVwsluVggnu1khnOxmhWhrnR0Q/baibsqoaYM3btxI46z+DwAXLlyojKk6utp3b29vre0HBwcrYwcOHKDbqumc33vvPRofGxuj8ePHj1fGNmzYQLe9/vrraXzr1q00fsstt1TGVB18/fr1NK76/FWtnF2v6h4Adc9IlVrJHhGjAM4BmAEwnZnDdfZnZkunFa/s/5CZJ1qwHzNbQv6b3awQdZM9AfwhIl6JiJ0LfUNE7IyIkYgYYX/3mtnSqvtr/B2ZORYRWwC8EBFvZ+ZL878hM3cB2AUAAwMD9RYeM7Om1Xplz8yxxsdJAM8AuL0VgzKz1ms62SNibURcfflzAF8DsL9VAzOz1qrza3w/gGcaNb/lAP4zM/+LbRARdL5tVa9mPeuq/1jV8FWc9V6z3mSg3pLLANDX10fjExMTlTE1B7mqVQ8NDdH4qVOnaJydU3W+N23aROOqV5/Nia/m8h8fH6fx/v5+Gt+2bRuNs+tN3bvAfi52vptO9sw8BODWZrc3s/Zy6c2sEE52s0I42c0K4WQ3K4ST3awQbW1xnZmZoa1/amrhOkvwKmqJ3rNnzzb93GqaalUG+uCDD2icHbcbbriBbqumqVbtlmrsrJVUlQWPHj1K46r1ly2FrfatpjW/7rrraFy1VLMW18nJSbota4mmS4vTvZrZFcPJblYIJ7tZIZzsZoVwspsVwsluVggnu1kh2lpn7+npoXVbViME6i3ZrGrhamphFlfTMatWTjVdl2q/Zcsmq7Gx+wcA3Tqs2nPZOVPLPata9ejoKI2z466mglbts+q4qmmy2fZq+m91f0IVv7KbFcLJblYIJ7tZIZzsZoVwspsVwsluVggnu1kh2r5kM6N6o9mUyqr2qOKqnszq+KrmqnrlVR1d3X/A6tF1e+l37NhB42qabFYzPn36NN1W3Ruhes7ZeVH3Vag6vNpeLSHOfja13DO7lumS6HSvZnbFcLKbFcLJblYIJ7tZIZzsZoVwspsVwsluVoi21tkzk9YIVd2U9V6req+ak17FWc+5WlJZ/Vyqn131w7O+7oMHD9Jtb72VL8S7ZcsWGlfLKh8/frwytm/fPrpt3XkCWK+9qoOrc6aeW12PbE57ta2qw1eRr+wR8WRETEbE/nmPbYyIFyLiQOMj79Q3s45bzK/xvwRw12ceewTAi5l5I4AXG1+bWReTyZ6ZLwGY+szD9wDY3fh8N4B7WzwuM2uxZt+g68/M8cbnxwD0V31jROyMiJGIGFF/m5rZ0qn9bnzO3Xlfefd9Zu7KzOHMHF69enXdpzOzJjWb7BMRMQgAjY982Ukz67hmk/05AA82Pn8QwLOtGY6ZLRVZZ4+IpwDcCWBzRBwB8CMAjwP4bUQ8BOAwgPsW+4SsPql6zlktXPWEK+vXr6dxNt+9qnuqmqyKT0199v3RxT+/mv88Imhcre+u3odh/ewTExN0W1aLBvRxZ3V2Vv8HdK+9mttdHfeBgYHKmLqW2fXC+tllsmfm/RWhr6ptzax7+HZZs0I42c0K4WQ3K4ST3awQTnazQnTVks1q+WA27bEqV6g2VKW3t7cytmrVKrrt2NhYreceHBykcXZn4jvvvEO3VWNXLazPPstvsRgfH6+MqWW2VTlUTbHNWkUnJ/l9YCdOnKBxVRZU55xdT6oEzXKI5YFf2c0K4WQ3K4ST3awQTnazQjjZzQrhZDcrhJPdrBBtrbPPzs7SlkhVV2Vtg6ruWdfJkycrY6rNU9Wy165dS+Nq6WLWyqnuP2A1WwDYu3cvjb/88ss0zmrhaprq7du307hqv2VtqOr+A1XDV+ekDnW9NMuv7GaFcLKbFcLJblYIJ7tZIZzsZoVwspsVwsluVoi219nVMrwM60++dOkS3VbVwtXUwkeOHKmMqeV/Vd2U9TYDeirp1157rTK2detWuq3q296zZw+Nq1V+2HFVx0VNc62OO5v/QB3zoaGhpvcNABcvXqRxdv+Dmlqc1fhZzK/sZoVwspsVwsluVggnu1khnOxmhXCymxXCyW5WiLbW2ZcvX0571lVtklH1e1XTPXToEI2z5YHV0sGq117Vi9U9Amwpa3VcRkdHaVzNYV5n2WR1TtRxUesMMGou/g0bNtD4u+++S+NqnQJWS1fHhR1Tdr7kK3tEPBkRkxGxf95jj0XEWETsbfy7W+3HzDprMb/G/xLAXQs8/tPMvK3x7/nWDsvMWk0me2a+BIDfr2lmXa/OG3QPR8TrjV/zK//AiYidETESESPnz5+v8XRmVkezyf5zAF8CcBuAcQA/rvrGzNyVmcOZOawmNzSzpdNUsmfmRGbOZOYsgF8AuL21wzKzVmsq2SNift3iGwD2V32vmXUHWWePiKcA3Algc0QcAfAjAHdGxG0AEsAogG8v5slmZmbo3O9qvW5Wh1fzfKu+bRVnNV013z3rwwd0LXzFihU0vnLlysqYundh48aNND4zM0Pjhw8fpnF2Tuv2q7NrCeC93aoOrv7kVP3s7JwAvM9fXS/snLE+eZnsmXn/Ag8/obYzs+7i22XNCuFkNyuEk92sEE52s0I42c0K0dYW18ykrX2sdQ/gbYHbtm2j246Pj9O4Kt2xlkc19e+xY8doXP3cqrTHbkNm7a9AveWgAV1iGhgYqIypc6Jae9XYWdlPnW/13Js3b6bxiYkJGmfLdKvpudnYWTnTr+xmhXCymxXCyW5WCCe7WSGc7GaFcLKbFcLJblaIttbZlTr1RTXlMatrAnrqYLb922+/TbdV9WQ1JfL+/Xy6AHZctm/fTrdV9WR13FRbMqvTqxZVdf+CqnWza0JdL2psqv1WtS1v2bKlMqZaf9k5qzWVtJldGZzsZoVwspsVwsluVggnu1khnOxmhXCymxWirXX2np4eWktn0+ACfEplVQ8+deoUjatllTdt2lQZU730ipqWWP1sbNpjNQ113eWiVd8263dX01SzXnhAT5PNrqdLly7V2reqo6t5BFgtve71UMWv7GaFcLKbFcLJblYIJ7tZIZzsZoVwspsVwsluVoi219lZ/VHVPln9Uc0DrmqTam521petltgdHh6m8R07dtC4qnWz3ms17/s111xD4+r+BHVvBDun6pgrqh7d29tbGVP3F6h9q3Oi5tNnx6XZOroiX9kjYigi/hgRb0bEGxHx3cbjGyPihYg40PjIZ38ws45azK/x0wC+n5k3A/g7AN+JiJsBPALgxcy8EcCLja/NrEvJZM/M8cx8tfH5OQBvAdgK4B4AuxvfthvAvUs1SDOr7696gy4ivgjgywD+DKA/My9PrnYMQH/FNjsjYiQiRtiaZGa2tBad7BGxDsDvAHwvMz81Q2LOrbC34Cp7mbkrM4czc5g1bJjZ0lpUskfECswl+q8z8/eNhyciYrARHwQwuTRDNLNWkKW3mOvFewLAW5n5k3mh5wA8CODxxsdn1b4yk051q9ox2Z8BqhSi9q2mc2atuapVk00bDOjpmBW2JLT6bUqVoPr6+mhctQazUqsq29Utj7E2VHXO1HOrcqsqBbPrSU0l3eySzYups38FwLcA7IuIvY3HHsVckv82Ih4CcBjAfYvYl5l1iEz2zPwTgKr/Lr7a2uGY2VLx7bJmhXCymxXCyW5WCCe7WSGc7GaFaGuLa0TQ2qpaRpe1RKqWQjU18LXXXkvj77//ftP7Vj9Xf/+CdxoventWb1a16FWrVtG4akNVx43VhFUtW9WqVa2c3X+gnlvdA6CW+Fbt2mzsqsWVtS17yWYzc7KblcLJblYIJ7tZIZzsZoVwspsVwsluVoi21tmnp6fp1MRqmVtWm1R91WrKZFXbZEs2q+V71XRcqqartmf98HV7wk+cOEHjaqpqdk7nJjiqpuYYUH3f7N4LdT2waagBXUdX1xOrh6tzwo4pOyZ+ZTcrhJPdrBBOdrNCONnNCuFkNyuEk92sEE52s0K0vZ+d1gFFvZnFVV+2omqbrK9b1XvPnDlD49PT0zSuxsZqvkNDQ3Rb1RN+/PhxGldjY/c/qF74uvdGsGtCXS/q3ga2TDag7z9g9xiosbH5E9h+/cpuVggnu1khnOxmhXCymxXCyW5WCCe7WSGc7GaFWMz67EMAfgWgH0AC2JWZP4uIxwD8M4DLhdhHM/N5tq+enh5aG1U1W9VjzKj129l62QAwNTVVGVNziN900000PjAwQOODg4NNx+vOvX7w4EEar9NzruYvUPs+efIkjR8+fLgypuYgUOsQqH52NscAwOcZWLFiBd12zZo1lTF2vhdzU800gO9n5qsRcTWAVyLihUbsp5n5r4vYh5l12GLWZx8HMN74/FxEvAVg61IPzMxa66/6mz0ivgjgywD+3Hjo4Yh4PSKejIgFf5eNiJ0RMRIRI+oWRDNbOotO9ohYB+B3AL6XmWcB/BzAlwDchrlX/h8vtF1m7srM4cwcZn9rmNnSWlSyR8QKzCX6rzPz9wCQmROZOZOZswB+AeD2pRummdUlkz3m3k59AsBbmfmTeY/Pfwv4GwD2t354ZtYqi3k3/isAvgVgX0TsbTz2KID7I+I2zJXjRgF8u+5g1q1bR+OsNKfaY1VZT5V5WHlNTbes2iEnJiZofP9+/v8oO26qpKjKPKpVU5WwWPuuau0dHx+ncfWzsbGrkqMqranSnNo/KzuqadFPnz7d1PMu5t34PwFYqFhKa+pm1l18B51ZIZzsZoVwspsVwsluVggnu1khnOxmhWj7VNKshsiWsQV4bZLVHgHd6qnq9KwOr2rVajrmo0eP0riaWpi1kaplkdXY1T0CqpWT3d+glpPu6+ujcbU9uwdA3Xeh6ujqnKg6PbvW1XOra7mKX9nNCuFkNyuEk92sEE52s0I42c0K4WQ3K4ST3awQoeqwLX2yiOMA5s/vuxkAbwbvnG4dW7eOC/DYmtXKsX0hMxdc67qtyf65J48Yyczhjg2A6Naxdeu4AI+tWe0am3+NNyuEk92sEJ1O9l0dfn6mW8fWreMCPLZmtWVsHf2b3czap9Ov7GbWJk52s0J0JNkj4q6I+L+IOBgRj3RiDFUiYjQi9kXE3ogY6fBYnoyIyYjYP++xjRHxQkQcaHzk60W3d2yPRcRY49jtjYi7OzS2oYj4Y0S8GRFvRMR3G4939NiRcbXluLX9b/aIWAbgHQD/COAIgD0A7s/MN9s6kAoRMQpgODM7fgNGRPw9gA8B/Coz/6bx2L8AmMrMxxv/UW7IzB90ydgeA/Bhp5fxbqxWNDh/mXEA9wL4J3Tw2JFx3Yc2HLdOvLLfDuBgZh7KzIsAfgPgng6Mo+tl5ksApj7z8D0Adjc+3425i6XtKsbWFTJzPDNfbXx+DsDlZcY7euzIuNqiE8m+FcAH874+gu5a7z0B/CEiXomInZ0ezAL6M/PyukjHAPR3cjALkMt4t9NnlhnvmmPXzPLndfkNus+7IzP/FsDXAXyn8etqV8q5v8G6qXa6qGW822WBZcb/opPHrtnlz+vqRLKPARia9/V1jce6QmaONT5OAngG3bcU9cTlFXQbHyc7PJ6/6KZlvBdaZhxdcOw6ufx5J5J9D4AbI2JbRKwE8E0Az3VgHJ8TEWsbb5wgItYC+Bq6bynq5wA82Pj8QQDPdnAsn9Ity3hXLTOODh+7ji9/nplt/wfgbsy9I/8ugB92YgwV47oBwP82/r3R6bEBeApzv9Zdwtx7Gw8B2ATgRQAHAPwPgI1dNLb/ALAPwOuYS6zBDo3tDsz9iv46gL2Nf3d3+tiRcbXluPl2WbNC+A06s0I42c0K4WQ3K4ST3awQTnazQjjZzQrhZDcrxP8DEcZnJCZBZTIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig = plt.figure()\n",
        "plt.imshow(list(model.parameters())[0].reshape(28,28).detach().numpy(), cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yD96j3ZC2MFe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyZptxKN2MLr",
        "outputId": "d42f09a2-d453-4dcf-d920-d0d21beeb667"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [59900/60000 (100%)]\tLoss: 0.564943\tAcc: 0.800000\n",
            "Test set: Avg. loss: 0.0055, Accuracy: 4926/10000 (49%), (ACC:0.750000)\n",
            "\n",
            "Train Epoch: 1 [59900/60000 (100%)]\tLoss: 0.494919\tAcc: 0.820000\n",
            "Test set: Avg. loss: 0.0048, Accuracy: 4926/10000 (49%), (ACC:0.760000)\n",
            "\n",
            "Train Epoch: 2 [59900/60000 (100%)]\tLoss: 0.454439\tAcc: 0.830000\n",
            "Test set: Avg. loss: 0.0044, Accuracy: 4926/10000 (49%), (ACC:0.750000)\n",
            "\n",
            "Train Epoch: 3 [59900/60000 (100%)]\tLoss: 0.427970\tAcc: 0.840000\n",
            "Test set: Avg. loss: 0.0042, Accuracy: 4926/10000 (49%), (ACC:0.760000)\n",
            "\n",
            "Train Epoch: 4 [59900/60000 (100%)]\tLoss: 0.409195\tAcc: 0.840000\n",
            "Test set: Avg. loss: 0.0040, Accuracy: 4926/10000 (49%), (ACC:0.740000)\n",
            "\n",
            "Train Epoch: 5 [59900/60000 (100%)]\tLoss: 0.395109\tAcc: 0.840000\n",
            "Test set: Avg. loss: 0.0039, Accuracy: 4926/10000 (49%), (ACC:0.740000)\n",
            "\n",
            "Train Epoch: 6 [59900/60000 (100%)]\tLoss: 0.384098\tAcc: 0.840000\n",
            "Test set: Avg. loss: 0.0038, Accuracy: 4926/10000 (49%), (ACC:0.740000)\n",
            "\n",
            "Train Epoch: 7 [59900/60000 (100%)]\tLoss: 0.375220\tAcc: 0.840000\n",
            "Test set: Avg. loss: 0.0037, Accuracy: 4926/10000 (49%), (ACC:0.740000)\n",
            "\n",
            "Train Epoch: 8 [59900/60000 (100%)]\tLoss: 0.367885\tAcc: 0.850000\n",
            "Test set: Avg. loss: 0.0037, Accuracy: 4926/10000 (49%), (ACC:0.740000)\n",
            "\n",
            "Train Epoch: 9 [59900/60000 (100%)]\tLoss: 0.361703\tAcc: 0.850000\n",
            "Test set: Avg. loss: 0.0036, Accuracy: 4926/10000 (49%), (ACC:0.740000)\n",
            "\n",
            "Train Epoch: 10 [59900/60000 (100%)]\tLoss: 0.356408\tAcc: 0.850000\n",
            "Test set: Avg. loss: 0.0036, Accuracy: 4926/10000 (49%), (ACC:0.750000)\n",
            "\n",
            "Train Epoch: 11 [59900/60000 (100%)]\tLoss: 0.351809\tAcc: 0.850000\n",
            "Test set: Avg. loss: 0.0035, Accuracy: 4926/10000 (49%), (ACC:0.750000)\n",
            "\n",
            "Train Epoch: 12 [59900/60000 (100%)]\tLoss: 0.347768\tAcc: 0.850000\n",
            "Test set: Avg. loss: 0.0035, Accuracy: 4926/10000 (49%), (ACC:0.760000)\n",
            "\n",
            "Train Epoch: 13 [59900/60000 (100%)]\tLoss: 0.344179\tAcc: 0.850000\n",
            "Test set: Avg. loss: 0.0034, Accuracy: 4926/10000 (49%), (ACC:0.760000)\n",
            "\n",
            "Train Epoch: 14 [59900/60000 (100%)]\tLoss: 0.340964\tAcc: 0.850000\n",
            "Test set: Avg. loss: 0.0034, Accuracy: 4926/10000 (49%), (ACC:0.760000)\n",
            "\n",
            "Train Epoch: 15 [59900/60000 (100%)]\tLoss: 0.338062\tAcc: 0.850000\n",
            "Test set: Avg. loss: 0.0034, Accuracy: 4926/10000 (49%), (ACC:0.760000)\n",
            "\n",
            "Train Epoch: 16 [59900/60000 (100%)]\tLoss: 0.335423\tAcc: 0.850000\n",
            "Test set: Avg. loss: 0.0034, Accuracy: 4926/10000 (49%), (ACC:0.760000)\n",
            "\n",
            "Train Epoch: 17 [59900/60000 (100%)]\tLoss: 0.333008\tAcc: 0.860000\n",
            "Test set: Avg. loss: 0.0033, Accuracy: 4926/10000 (49%), (ACC:0.760000)\n",
            "\n",
            "Train Epoch: 18 [59900/60000 (100%)]\tLoss: 0.330785\tAcc: 0.860000\n",
            "Test set: Avg. loss: 0.0033, Accuracy: 4926/10000 (49%), (ACC:0.760000)\n",
            "\n",
            "Train Epoch: 19 [59900/60000 (100%)]\tLoss: 0.328730\tAcc: 0.860000\n",
            "Test set: Avg. loss: 0.0033, Accuracy: 4926/10000 (49%), (ACC:0.760000)\n",
            "\n",
            "Train Epoch: 20 [59900/60000 (100%)]\tLoss: 0.326820\tAcc: 0.870000\n",
            "Test set: Avg. loss: 0.0033, Accuracy: 4926/10000 (49%), (ACC:0.770000)\n",
            "\n",
            "Train Epoch: 21 [59900/60000 (100%)]\tLoss: 0.325038\tAcc: 0.870000\n",
            "Test set: Avg. loss: 0.0033, Accuracy: 4926/10000 (49%), (ACC:0.770000)\n",
            "\n",
            "Train Epoch: 22 [59900/60000 (100%)]\tLoss: 0.323368\tAcc: 0.870000\n",
            "Test set: Avg. loss: 0.0033, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 23 [59900/60000 (100%)]\tLoss: 0.321798\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0032, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 24 [59900/60000 (100%)]\tLoss: 0.320317\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0032, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 25 [59900/60000 (100%)]\tLoss: 0.318916\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0032, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 26 [59900/60000 (100%)]\tLoss: 0.317587\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0032, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 27 [59900/60000 (100%)]\tLoss: 0.316323\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0032, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 28 [59900/60000 (100%)]\tLoss: 0.315117\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0032, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 29 [59900/60000 (100%)]\tLoss: 0.313965\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0032, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 30 [59900/60000 (100%)]\tLoss: 0.312862\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0032, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 31 [59900/60000 (100%)]\tLoss: 0.311803\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0032, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 32 [59900/60000 (100%)]\tLoss: 0.310786\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 33 [59900/60000 (100%)]\tLoss: 0.309806\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 34 [59900/60000 (100%)]\tLoss: 0.308861\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 35 [59900/60000 (100%)]\tLoss: 0.307949\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 36 [59900/60000 (100%)]\tLoss: 0.307067\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 37 [59900/60000 (100%)]\tLoss: 0.306213\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 38 [59900/60000 (100%)]\tLoss: 0.305385\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 39 [59900/60000 (100%)]\tLoss: 0.304581\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 40 [59900/60000 (100%)]\tLoss: 0.303801\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 41 [59900/60000 (100%)]\tLoss: 0.303041\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 42 [59900/60000 (100%)]\tLoss: 0.302303\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 43 [59900/60000 (100%)]\tLoss: 0.301583\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 44 [59900/60000 (100%)]\tLoss: 0.300881\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 45 [59900/60000 (100%)]\tLoss: 0.300197\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 46 [59900/60000 (100%)]\tLoss: 0.299528\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0031, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 47 [59900/60000 (100%)]\tLoss: 0.298875\tAcc: 0.880000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 48 [59900/60000 (100%)]\tLoss: 0.298237\tAcc: 0.890000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 49 [59900/60000 (100%)]\tLoss: 0.297612\tAcc: 0.890000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 50 [59900/60000 (100%)]\tLoss: 0.297001\tAcc: 0.890000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 51 [59900/60000 (100%)]\tLoss: 0.296402\tAcc: 0.890000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 52 [59900/60000 (100%)]\tLoss: 0.295816\tAcc: 0.890000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 53 [59900/60000 (100%)]\tLoss: 0.295241\tAcc: 0.890000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 54 [59900/60000 (100%)]\tLoss: 0.294677\tAcc: 0.890000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 55 [59900/60000 (100%)]\tLoss: 0.294124\tAcc: 0.890000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.780000)\n",
            "\n",
            "Train Epoch: 56 [59900/60000 (100%)]\tLoss: 0.293581\tAcc: 0.890000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 57 [59900/60000 (100%)]\tLoss: 0.293049\tAcc: 0.890000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 58 [59900/60000 (100%)]\tLoss: 0.292525\tAcc: 0.890000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.790000)\n",
            "\n",
            "Train Epoch: 59 [59900/60000 (100%)]\tLoss: 0.292011\tAcc: 0.890000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.800000)\n",
            "\n",
            "Train Epoch: 60 [59900/60000 (100%)]\tLoss: 0.291506\tAcc: 0.900000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.800000)\n",
            "\n",
            "Train Epoch: 61 [59900/60000 (100%)]\tLoss: 0.291009\tAcc: 0.900000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.800000)\n",
            "\n",
            "Train Epoch: 62 [59900/60000 (100%)]\tLoss: 0.290520\tAcc: 0.910000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.800000)\n",
            "\n",
            "Train Epoch: 63 [59900/60000 (100%)]\tLoss: 0.290040\tAcc: 0.910000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.800000)\n",
            "\n",
            "Train Epoch: 64 [59900/60000 (100%)]\tLoss: 0.289567\tAcc: 0.910000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.800000)\n",
            "\n",
            "Train Epoch: 65 [59900/60000 (100%)]\tLoss: 0.289102\tAcc: 0.910000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.800000)\n",
            "\n",
            "Train Epoch: 66 [59900/60000 (100%)]\tLoss: 0.288644\tAcc: 0.910000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.800000)\n",
            "\n",
            "Train Epoch: 67 [59900/60000 (100%)]\tLoss: 0.288193\tAcc: 0.910000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.800000)\n",
            "\n",
            "Train Epoch: 68 [59900/60000 (100%)]\tLoss: 0.287748\tAcc: 0.910000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.810000)\n",
            "\n",
            "Train Epoch: 69 [59900/60000 (100%)]\tLoss: 0.287311\tAcc: 0.910000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.810000)\n",
            "\n",
            "Train Epoch: 70 [59900/60000 (100%)]\tLoss: 0.286880\tAcc: 0.910000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.810000)\n",
            "\n",
            "Train Epoch: 71 [59900/60000 (100%)]\tLoss: 0.286455\tAcc: 0.910000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.810000)\n",
            "\n",
            "Train Epoch: 72 [59900/60000 (100%)]\tLoss: 0.286037\tAcc: 0.910000\n",
            "Test set: Avg. loss: 0.0030, Accuracy: 4926/10000 (49%), (ACC:0.810000)\n",
            "\n",
            "Train Epoch: 73 [59900/60000 (100%)]\tLoss: 0.285624\tAcc: 0.910000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.810000)\n",
            "\n",
            "Train Epoch: 74 [59900/60000 (100%)]\tLoss: 0.285217\tAcc: 0.910000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.820000)\n",
            "\n",
            "Train Epoch: 75 [59900/60000 (100%)]\tLoss: 0.284816\tAcc: 0.910000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.820000)\n",
            "\n",
            "Train Epoch: 76 [59900/60000 (100%)]\tLoss: 0.284420\tAcc: 0.910000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.820000)\n",
            "\n",
            "Train Epoch: 77 [59900/60000 (100%)]\tLoss: 0.284030\tAcc: 0.910000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.820000)\n",
            "\n",
            "Train Epoch: 78 [59900/60000 (100%)]\tLoss: 0.283645\tAcc: 0.910000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.820000)\n",
            "\n",
            "Train Epoch: 79 [59900/60000 (100%)]\tLoss: 0.283265\tAcc: 0.910000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.820000)\n",
            "\n",
            "Train Epoch: 80 [59900/60000 (100%)]\tLoss: 0.282890\tAcc: 0.900000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.820000)\n",
            "\n",
            "Train Epoch: 81 [59900/60000 (100%)]\tLoss: 0.282521\tAcc: 0.900000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.820000)\n",
            "\n",
            "Train Epoch: 82 [59900/60000 (100%)]\tLoss: 0.282155\tAcc: 0.900000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.820000)\n",
            "\n",
            "Train Epoch: 83 [59900/60000 (100%)]\tLoss: 0.281795\tAcc: 0.900000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.820000)\n",
            "\n",
            "Train Epoch: 84 [59900/60000 (100%)]\tLoss: 0.281439\tAcc: 0.900000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.820000)\n",
            "\n",
            "Train Epoch: 85 [59900/60000 (100%)]\tLoss: 0.281088\tAcc: 0.900000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.820000)\n",
            "\n",
            "Train Epoch: 86 [59900/60000 (100%)]\tLoss: 0.280741\tAcc: 0.900000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.820000)\n",
            "\n",
            "Train Epoch: 87 [59900/60000 (100%)]\tLoss: 0.280398\tAcc: 0.900000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.820000)\n",
            "\n",
            "Train Epoch: 88 [59900/60000 (100%)]\tLoss: 0.280060\tAcc: 0.900000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.820000)\n",
            "\n",
            "Train Epoch: 89 [59900/60000 (100%)]\tLoss: 0.279726\tAcc: 0.900000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.820000)\n",
            "\n",
            "Train Epoch: 90 [59900/60000 (100%)]\tLoss: 0.279395\tAcc: 0.900000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.820000)\n",
            "\n",
            "Train Epoch: 91 [59900/60000 (100%)]\tLoss: 0.279069\tAcc: 0.900000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.820000)\n",
            "\n",
            "Train Epoch: 92 [59900/60000 (100%)]\tLoss: 0.278747\tAcc: 0.900000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.810000)\n",
            "\n",
            "Train Epoch: 93 [59900/60000 (100%)]\tLoss: 0.278429\tAcc: 0.900000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.810000)\n",
            "\n",
            "Train Epoch: 94 [59900/60000 (100%)]\tLoss: 0.278114\tAcc: 0.900000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.810000)\n",
            "\n",
            "Train Epoch: 95 [59900/60000 (100%)]\tLoss: 0.277803\tAcc: 0.900000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.810000)\n",
            "\n",
            "Train Epoch: 96 [59900/60000 (100%)]\tLoss: 0.277496\tAcc: 0.890000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.810000)\n",
            "\n",
            "Train Epoch: 97 [59900/60000 (100%)]\tLoss: 0.277192\tAcc: 0.890000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.810000)\n",
            "\n",
            "Train Epoch: 98 [59900/60000 (100%)]\tLoss: 0.276891\tAcc: 0.890000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.810000)\n",
            "\n",
            "Train Epoch: 99 [59900/60000 (100%)]\tLoss: 0.276595\tAcc: 0.890000\n",
            "Test set: Avg. loss: 0.0029, Accuracy: 4926/10000 (49%), (ACC:0.810000)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.001\n",
        "epochs = 100\n",
        "\n",
        "log_interval = 100\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5, eta_min=0)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=-0.35, patience=5, threshold=1e-2)\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "train_acc = 0\n",
        "for epoch in range(epochs) :\n",
        "  for batch_idx, (data, y) in enumerate(train_loader):\n",
        "    data = Variable(data.view(-1,28 * 28))\n",
        "    y = Variable(y.view(-1))\n",
        "    y = (y%2).float()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(data)\n",
        "    loss = loss_fn(y_pred.view_as(y), y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  # scheduler.step()\n",
        "  y_lb = (y_pred.data>0.5)*1\n",
        "  train_acc = y_lb.eq(y.view_as(y_lb)).sum() / len(y_lb)\n",
        "  print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAcc: {:.6f}'.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item(), train_acc )) \n",
        "  train_losses.append(loss.item())\n",
        "\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  test_acc = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data = Variable(data.view(-1,28 * 28))\n",
        "      target = Variable(target.view(-1))\n",
        "      target = (target%2).float()\n",
        "      \n",
        "      y_pred = model(data)\n",
        "      y_lb = (y_pred.data>0.5)*1\n",
        "\n",
        "      test_loss += loss_fn(y_pred.view_as(target), target)\n",
        "      pred = y_pred.data.max(1, keepdim=True)[1]\n",
        "      \n",
        "      test_acc = y_lb.eq(target.view_as(y_lb)).sum() / len(y_lb)\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_losses.append(test_loss)\n",
        "  print('Test set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), (ACC:{:.6f})\\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset), test_acc))\n",
        "\n",
        "scheduler.step(test_acc)\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfhet97DjVN8"
      },
      "source": [
        "# 3. MNIST+SOFTMAX, MNIST+NN, dropout, regularization, etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC2KVb3y_Oyz"
      },
      "source": [
        "1. MNIST + No hidden layer + 10 output layer(Softmax)\n",
        "2. MNIST + N hidden layer + 10 output layer\n",
        "    1. 리스트 형태로 숫자 받아서 히든 레이어 만들기\n",
        "    ex) input : [128,64,32]\n",
        "\n",
        "    Model : 784 - 128 - 64 - 32 - 10\n",
        "    \n",
        "    Activation function은 자유롭게 쓰고 싶은거 사용\n",
        "3. 2.+ dropout layer\n",
        "4. 2.+ kernal regularizarion\n",
        "    1. L1, L2 두개 구현하기, weight decay 와 앞에 두개의 차이\n",
        "5. 레이어 initialization 방법 적용하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "z0eQ__VljbWy",
        "outputId": "e89d95a0-577e-4f8c-975a-d9ed7d2ff997"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "학습을 진행하는 기기: cuda:0 \n",
            "\n",
            "train_set (shape) :  torch.Size([100, 1, 28, 28])\n",
            "test_loader (shape) :  torch.Size([100, 1, 28, 28])\n",
            "x.shape :  torch.Size([1, 28, 28])\n",
            "y :  5\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transfroms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "print(USE_CUDA)\n",
        "\n",
        "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
        "print('학습을 진행하는 기기:',device ,\"\\n\")\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "# MNIST 데이터셋 로드\n",
        "train_set = torchvision.datasets.MNIST(\n",
        "    root = './data/MNIST',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = transfroms.Compose([\n",
        "        transfroms.ToTensor() # 데이터를 0에서 255까지 있는 값을 0에서 1사이 값으로 변환\n",
        "    ])\n",
        ")\n",
        "test_set = torchvision.datasets.MNIST(\n",
        "    root = './data/MNIST',\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = transfroms.Compose([\n",
        "        transfroms.ToTensor() # 데이터를 0에서 255까지 있는 값을 0에서 1사이 값으로 변환\n",
        "    ])\n",
        ")\n",
        " \n",
        "# train_loader, test_loader 생성\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
        " \n",
        "\n",
        "# input size를 알기 위해서\n",
        "examples = enumerate(train_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "print(\"train_set (shape) : \", example_data.shape)\n",
        "\n",
        "examples = enumerate(test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "print(\"test_loader (shape) : \",example_data.shape)\n",
        "\n",
        "x = train_set[0][0]\n",
        "y = train_set[0][1]\n",
        "print(\"x.shape : \", x.shape)\n",
        "print(\"y : \", y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXy8fqlOlIRL",
        "outputId": "3885a9cb-2aae-4f7e-815f-0bfbb5e4c69d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<<<<< model1 >>>>>\n",
            "Net(\n",
            "  (layers): ModuleList()\n",
            "  (out): Linear(in_features=784, out_features=10, bias=True)\n",
            ")\n",
            "<<<<< model2 >>>>>\n",
            "Net(\n",
            "  (layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (1): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
            "  )\n",
            "  (out): Linear(in_features=32, out_features=10, bias=True)\n",
            ")\n",
            "<<<<< model3 >>>>>\n",
            "Net(\n",
            "  (layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
            "    (1): Dropout(p=0.2, inplace=False)\n",
            "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (3): Dropout(p=0.2, inplace=False)\n",
            "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (5): Dropout(p=0.2, inplace=False)\n",
            "  )\n",
            "  (out): Linear(in_features=32, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self,  layers_data: list, dr_rate = 0):\n",
        "        super(Net, self).__init__()\n",
        "        input_size = 28*28\n",
        "        output_size = 10\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        for size in layers_data:\n",
        "            self.layers.append(nn.Linear(input_size, size))\n",
        "            input_size = size  # For the next layer\n",
        "\n",
        "            if dr_rate != 0  :\n",
        "              self.layers.append(nn.Dropout(p=dr_rate))\n",
        "\n",
        "        self.out = nn.Linear(input_size, output_size)\n",
        "    \n",
        "        if USE_CUDA:\n",
        "            self.out = self.out.cuda()\n",
        "\n",
        "\n",
        "    # def _init_weights(self, module):\n",
        "    #     if isinstance(module, nn.Linear):\n",
        "    #         module.weight.data.normal_(mean=0.0, std=1.0)\n",
        "    #         if module.bias is not None:\n",
        "    #             module.bias.data.zero_()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784) # Flatten the data(n, 1, 28, 28) - > (n, 784)\n",
        "        for layer in self.layers:\n",
        "            # x = layer(x)\n",
        "            x = F.relu(layer(x))\n",
        "            \n",
        "        output= F.softmax(self.out(x), dim=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "print(\"<<<<< model1 >>>>>\")\n",
        "layers_data  = []\n",
        "model = Net(layers_data).to(device)\n",
        "print(model)\n",
        "\n",
        "print(\"<<<<< model2 >>>>>\")\n",
        "layers_data  = [128,64,32]\n",
        "model = Net(layers_data).to(device)\n",
        "print(model)\n",
        "\n",
        "print(\"<<<<< model3 >>>>>\")\n",
        "layers_data  = [128,64,32]\n",
        "model = Net(layers_data, 0.2).to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oZHbTykAlIO-"
      },
      "outputs": [],
      "source": [
        "def training(model, epochs=100, learning_rate=0.001, norm_v=-1, norm_c=0.3):\n",
        "  norm = 0\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "\n",
        "  train_acc = 0\n",
        "  for epoch in range(epochs) :\n",
        "    for batch_idx, (data, y) in enumerate(train_loader):\n",
        "      data, y = data.to(device), y.to(device)\n",
        "      # data = Variable(data.view(-1,28 * 28))\n",
        "      y = Variable(y.view(-1))\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "      y_pred = model(data)       \n",
        "\n",
        "      loss = loss_fn(y_pred, y)\n",
        "\n",
        "      if norm_v == 1 :\n",
        "        norm = sum(map(abs, torch.argmax(y_pred,dim=1)  - y))  / len(y)\n",
        "      elif norm_v == 2 :\n",
        "        norm = sum(pow( ( torch.argmax(y_pred,dim=1)  - y ) , 2) )  / len(y) \n",
        "        # regularity =  torch.norm(model.fc.weight, p=p)\n",
        "      loss = loss + norm_c*norm\n",
        "    \n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    \n",
        "    y_lb = torch.argmax(y_pred,dim=1)\n",
        "    train_acc = y_lb.eq(y.view_as(y_lb)).sum() / len(y_lb)\n",
        "    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAcc: {:.6f}'.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item(), train_acc )) \n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "\n",
        "    if epoch % 10 == 0 :\n",
        "      test_loss = 0\n",
        "      test_acc = 0\n",
        "      with torch.no_grad():\n",
        "        for data, y in test_loader:\n",
        "          data, y = data.to(device), y.to(device)\n",
        "          y = Variable(y.view(-1))\n",
        "            \n",
        "          y_pred = model(data)\n",
        "          y_lb = torch.argmax(y_pred,dim=1)\n",
        "          # pred = y_pred.data.max(1, keepdim=True)[1]\n",
        "\n",
        "          test_loss += loss_fn(y_pred, y).float()\n",
        "          test_acc += y_lb.eq(y.view_as(y_lb)).sum().float()\n",
        "      test_loss /= len(test_loader.dataset)\n",
        "      test_acc /= len(test_loader.dataset)\n",
        "      print('Test set: Avg. Loss: {:.4f}, (ACC:{:.6f})\\n'.format(test_loss, test_acc))\n",
        "\n",
        "  return model, [train_acc.item(), train_losses[-1], test_acc.item(), test_loss.item()] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hyf4To5JEh6w",
        "outputId": "48602f11-acaa-442c-9c45-7c6200f5d1fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------- Model1 --------------------\n"
          ]
        }
      ],
      "source": [
        "layers_data  = []\n",
        "model1 = Net(layers_data)\n",
        "\n",
        "layers_data  = [128,64,32]\n",
        "model2 = Net(layers_data)\n",
        "\n",
        "layers_data  = [128,64,32]\n",
        "model3 = Net(layers_data, 0.2)\n",
        "\n",
        "print(\"-------------------- Model1 --------------------\")\n",
        "md1, result1 = training(model1, epochs=100, learning_rate=0.005)\n",
        "\n",
        "print(\"-------------------- Model2 --------------------\")\n",
        "md2, result2 = training(model2, epochs=100, learning_rate=0.005)\n",
        "\n",
        "print(\"-------------------- Model3 --------------------\")\n",
        "md3, result3 = training(model3, epochs=100, learning_rate=0.005)\n",
        "\n",
        "print(\"-------------------- Model4 --------------------\")\n",
        "layers_data  = [128,64,32]\n",
        "model3 = Net(layers_data, 0.2)\n",
        "md4, result4 = training(model3, epochs=100, learning_rate=0.005, norm_v=1, norm_c=0.2)\n",
        "\n",
        "print(\"-------------------- Model5 --------------------\")\n",
        "layers_data  = [128,64,32]\n",
        "model3 = Net(layers_data, 0.2)\n",
        "md5, result5 = training(model3, epochs=100, learning_rate=0.005, norm_v=2, norm_c=0.2)\n",
        "\n",
        "\n",
        "print(\"-------------------- Result --------------------\")\n",
        "print( \"model1 : TrAcc:{:.5f} TestAcc:{:.5f}, TrLoss{:.5f} TestLoss{:.5f} \".format(result1[0],result1[2], result1[1],result1[3] ) )\n",
        "print( \"model2 : TrAcc:{:.5f} TestAcc:{:.5f}, TrLoss{:.5f} TestLoss{:.5f} \".format(result2[0],result2[2], result2[1],result2[3] ) )\n",
        "print( \"model3 : TrAcc:{:.5f} TestAcc:{:.5f}, TrLoss{:.5f} TestLoss{:.5f} \".format(result3[0],result3[2], result3[1],result3[3] ) )\n",
        "print( \"model4 : TrAcc:{:.5f} TestAcc:{:.5f}, TrLoss{:.5f} TestLoss{:.5f} \".format(result4[0],result4[2], result4[1],result4[3] ) )\n",
        "print( \"model5 : TrAcc:{:.5f} TestAcc:{:.5f}, TrLoss{:.5f} TestLoss{:.5f} \".format(result5[0],result5[2], result5[1],result5[3] ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnIVzmF3EiB3"
      },
      "outputs": [],
      "source": [
        "# epochs=100, learning_rate=0.001\n",
        "# -------------------- Result --------------------\n",
        "# model1 : TrAcc:0.51900 TestAcc:0.51250, TrLoss2.16309 TestLoss0.00217 \n",
        "# model2 : TrAcc:0.15800 TestAcc:0.17150, TrLoss2.29884 TestLoss0.00230 \n",
        "# model3 : TrAcc:0.19600 TestAcc:0.17470, TrLoss2.29824 TestLoss0.00230 \n",
        "# model4 : TrAcc:0.25800 TestAcc:0.24920, TrLoss3.03882 TestLoss0.00230 \n",
        "# model5 : TrAcc:0.16200 TestAcc:0.13340, TrLoss5.56107 TestLoss0.00230 \n",
        "\n",
        "\n",
        "# epochs=200, learning_rate=0.003\n",
        "# -------------------- Result --------------------\n",
        "# model1 : TrAcc:0.83500 TestAcc:0.78780, TrLoss1.73969 TestLoss0.00179 \n",
        "# model2 : TrAcc:0.47500 TestAcc:0.44470, TrLoss2.03771 TestLoss0.00205 \n",
        "# model3 : TrAcc:0.35900 TestAcc:0.35720, TrLoss2.14618 TestLoss0.00214 \n",
        "# model4 : TrAcc:0.48400 TestAcc:0.44800, TrLoss2.65976 TestLoss0.00207 \n",
        "# model5 : TrAcc:0.44000 TestAcc:0.40960, TrLoss5.73952 TestLoss0.00219 \n",
        "\n",
        "# epochs=300, learning_rate=0.005, norm_c=0.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzwVTTAHEiE1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vxxNx7cEiHp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xVoBRvHClna"
      },
      "outputs": [],
      "source": [
        "import torch.nn.init as init\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_uniform(m.weight)\n",
        "        m.bias.data.fill_(0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0_tghnlC16a",
        "outputId": "5515ad64-c827-492b-f20b-87b802408ea2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  from ipykernel import kernelapp as app\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Net(\n",
              "  (layers): ModuleList(\n",
              "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
              "    (1): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (3): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (out): Linear(in_features=32, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model3.apply(init_weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpHrVNmllIJM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoTMa0smlIGk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQNKKPiC8AuJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuR_KevY7_Lt"
      },
      "source": [
        "# 새 섹션"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyW8kOQi8AZb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQmf2gaUja3g"
      },
      "source": [
        "# EX) 참고 예제  (1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-3vTzI6641M",
        "outputId": "76ec09da-2433-4fcb-e414-2a8eac46426b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu is available\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### ref1. https://yong0810.tistory.com/17\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transfroms\n",
        " \n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "print(device + \" is available\")\n",
        " \n",
        "learning_rate = 0.001\n",
        "batch_size = 100\n",
        "num_classes = 10\n",
        "epochs = 5\n",
        " \n",
        "# MNIST 데이터셋 로드\n",
        "train_set = torchvision.datasets.MNIST(\n",
        "    root = './data/MNIST',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = transfroms.Compose([\n",
        "        transfroms.ToTensor() # 데이터를 0에서 255까지 있는 값을 0에서 1사이 값으로 변환\n",
        "    ])\n",
        ")\n",
        "test_set = torchvision.datasets.MNIST(\n",
        "    root = './data/MNIST',\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = transfroms.Compose([\n",
        "        transfroms.ToTensor() # 데이터를 0에서 255까지 있는 값을 0에서 1사이 값으로 변환\n",
        "    ])\n",
        ")\n",
        " \n",
        "# train_loader, test_loader 생성\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
        " \n",
        "# input size를 알기 위해서\n",
        "examples = enumerate(train_set)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "example_data.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2naDQ9K7T_D"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "  def __init__(self): # layer 정의\n",
        "        super(ConvNet, self).__init__()\n",
        "\n",
        "        # input size = 28x28 \n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5) # input channel = 1, filter = 10, kernel size = 5, zero padding = 0, stribe = 1\n",
        "        # ((W-K+2P)/S)+1 공식으로 인해 ((28-5+0)/1)+1=24 -> 24x24로 변환\n",
        "        # maxpooling하면 12x12\n",
        "  \n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5) # input channel = 1, filter = 10, kernel size = 5, zero padding = 0, stribe = 1\n",
        "        # ((12-5+0)/1)+1=8 -> 8x8로 변환\n",
        "        # maxpooling하면 4x4\n",
        "\n",
        "        self.drop2D = nn.Dropout2d(p=0.25, inplace=False) # 랜덤하게 뉴런을 종료해서 학습을 방해해 학습이 학습용 데이터에 치우치는 현상을 막기 위해 사용\n",
        "        self.mp = nn.MaxPool2d(2)  # 오버피팅을 방지하고, 연산에 들어가는 자원을 줄이기 위해 maxpolling\n",
        "        self.fc1 = nn.Linear(320,100) # 4x4x20 vector로 flat한 것을 100개의 출력으로 변경\n",
        "        self.fc2 = nn.Linear(100,10) # 100개의 출력을 10개의 출력으로 변경\n",
        "\n",
        "  def forward(self, x):\n",
        "        x = F.relu(self.mp(self.conv1(x))) # convolution layer 1번에 relu를 씌우고 maxpool, 결과값은 12x12x10\n",
        "        x = F.relu(self.mp(self.conv2(x))) # convolution layer 2번에 relu를 씌우고 maxpool, 결과값은 4x4x20\n",
        "        x = self.drop2D(x)\n",
        "        x = x.view(x.size(0), -1) # flat\n",
        "        x = self.fc1(x) # fc1 레이어에 삽입\n",
        "        x = self.fc2(x) # fc2 레이어에 삽입\n",
        "        return F.log_softmax(x) # fully-connected layer에 넣고 logsoftmax 적용\n",
        " \n",
        "model = ConvNet().to(device) # CNN instance 생성\n",
        "# Cost Function과 Optimizer 선택\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z_h-Sj47Xbu",
        "outputId": "03e4389b-9b7c-458c-ae9c-7f43985d47cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch:    1] cost = 0.31739229\n",
            "[Epoch:    2] cost = 0.114715569\n",
            "[Epoch:    3] cost = 0.0867257565\n",
            "[Epoch:    4] cost = 0.0752806887\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs): # epochs수만큼 반복\n",
        "    avg_cost = 0\n",
        "\n",
        "    for data, target in train_loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad() # 모든 model의 gradient 값을 0으로 설정\n",
        "        hypothesis = model(data) # 모델을 forward pass해 결과값 저장 \n",
        "        cost = criterion(hypothesis, target) # output과 target의 loss 계산\n",
        "        cost.backward() # backward 함수를 호출해 gradient 계산\n",
        "        optimizer.step() # 모델의 학습 파라미터 갱신\n",
        "        avg_cost += cost / len(train_loader) # loss 값을 변수에 누적하고 train_loader의 개수로 나눔 = 평균\n",
        "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))\n",
        " \n",
        "# test\n",
        "model.eval() # evaluate mode로 전환 dropout 이나 batch_normalization 해제 \n",
        "with torch.no_grad(): # grad 해제 \n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for data, target in test_loader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        out = model(data)\n",
        "        preds = torch.max(out.data, 1)[1] # 출력이 분류 각각에 대한 값으로 나타나기 때문에, 가장 높은 값을 갖는 인덱스를 추출\n",
        "        total += len(target) # 전체 클래스 개수 \n",
        "        correct += (preds==target).sum().item() # 예측값과 실제값이 같은지 비교\n",
        "        \n",
        "    print('Test Accuracy: ', 100.*correct/total, '%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "C-z_H2MehhPz",
        "-7sKIqnyhkBd",
        "5_bseDK6yGb7",
        "Zhr_q-pDX3ZV",
        "RuR_KevY7_Lt",
        "FQmf2gaUja3g"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}